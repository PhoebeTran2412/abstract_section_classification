# -*- coding: utf-8 -*-
"""[DEMO DATA PREPROCESSING] Edited_BERT_Ver_Abstract_Section_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l3fQn91sOx7KbsLBiIPls_Aaw9CvAkFY
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras import layers
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

!pip install --quiet tensorflow-text
!pip install tensorflow_text
import tensorflow_text as text

import os

# Install Transformers library 
!pip install transformers
!pip install bert-for-tf2

import seaborn as sns
import matplotlib.pyplot as plt

plt.style.use('dark_background')

"""The input data is in list, already.

Defined as: "input_list"
"""

input_list = ['This RCT examined the efficacy of a manualized social intervention for children with HFASDs',
              'Participants were randomly assigned to treatment or wait-list conditions. Treatment included instruction and therapeutic activities targeting social skills, face-emotion recognition, interest expansion, and interpretation of non-literal language. A response-cost program was applied to reduce problem behaviors and foster skills acquisition'
              'Significant treatment effects were found for five of seven primary outcome measures (parent ratings and direct child measures)',
              'Secondary measures based on staff ratings (treatment group only) corroborated gains reported by parents',
              'High levels of parent, child and staff satisfaction were reported, along with high levels of treatment fidelity',
              'Standardized effect size estimates were primarily in the medium and large ranges and favored the treatment group']

"""## I. TOKEN-LEVEL DATA"""

token_inputs = []
for sequence in input_list:
    token_inputs.append(sequence.lower())

token_inputs

module = "https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1"
bert_layer = hub.KerasLayer(module, trainable=True)

from bert import bert_tokenization

vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)

def bert_encode(texts, tokenizer, max_len=512):
    all_tokens = []
    all_masks = []
    all_segments = []
    
    for text in texts:
        text = tokenizer.tokenize(text)
            
        text = text[:max_len-2]
        input_sequence = ["[CLS]"] + text + ["[SEP]"]
        pad_len = max_len - len(input_sequence)
        
        tokens = tokenizer.convert_tokens_to_ids(input_sequence)
        tokens += [0] * pad_len
        pad_masks = [1] * len(input_sequence) + [0] * pad_len
        segment_ids = [0] * max_len
        
        all_tokens.append(tokens)
        all_masks.append(pad_masks)
        all_segments.append(segment_ids)
    
    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)

token_bert_inputs = bert_encode(token_inputs, tokenizer, max_len=128)

token_bert_inputs

"""## II. CHARACTER-LEVEL DATA"""

# Create a character-level tokenizer
# Create a function to split sentences into characters
def split_characters(text):
    text_split_list = ' '.join(list(text))
    return text_split_list

# Apply split_character function to train dataset
character_inputs = [split_characters(sentence) for sentence in token_inputs]

# Convert character_list into numpy array
character_inputs = np.array(character_inputs)

character_inputs

"""## III. LINE_NUMBER AND TOTAL_LINE DATA

#### 1. line_number
"""

line_number = []
for line_index, _ in enumerate(input_list):
    line_number.append(line_index)

line_number_encoded = np.array(tf.one_hot(line_number, depth=15))

line_number_encoded

"""#### 2. total_line"""

total_line = []
for line in range(len(input_list)):
    total_line.append(len(input_list))

total_line_encoded = np.array(tf.one_hot(total_line, depth=20))

total_line_encoded

"""## MODEL INPUT DATASET"""

model_inputs = [token_bert_inputs, character_inputs, line_number_encoded, total_line_encoded]

model_inputs

"""## MODEL PREDICTION"""

# Load model
loaded_model = tf.keras.models.load_model('/content/gdrive/MyDrive/abstract_section_classification/model/Abstract_Section_Classification')

# Use model to predict the demo data
model_pred_proba = loaded_model.predict(model_inputs) # token_character_val_dataset is required to have the same format as the training dataset
model_pred_proba

# Convert probability predictions into labels
model_preds = tf.argmax(model_pred_proba, axis=1)
model_preds

# Create a function to return and display results
def display_results(model_preds_list, input_list):
    predictions = []
    for prediction in model_preds_list:
        if prediction == '0':
            predictions.append('BACKGROUND')

        elif prediction == '1':
            predictions.append('CONCLUSIONS')

        elif prediction == '2':
            predictions.append('METHODS')

        elif prediction == '3':
            predictions.append('OBJECTIVE')

        elif prediction == '4':
            predictions.append('RESULTS')
    
    results = []
    for index in range(len(model_preds_list)):
        results.append((model_preds_list[index], input_list[index]))

    return results

"""### DISCOVER MORE:

1. Training, Validation, and Testing data preparation: https://colab.research.google.com/drive/1rN-BfkA5Vc5nMToka_Pyr1q1DtxlXxki#scrollTo=FhIyNphGtFAY

2. Model creation and training: https://colab.research.google.com/drive/1ZrEdJBmeU0FDtNN5F_s0eU6yqvAEIPMt#scrollTo=d0etEZ_Mpqw1&uniqifier=1

3. Demo data preprocessing and model prediction
https://colab.research.google.com/drive/1l3fQn91sOx7KbsLBiIPls_Aaw9CvAkFY#scrollTo=vS0raMb7tNqR
"""